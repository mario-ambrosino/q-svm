#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\begin_modules
algorithm2e
theorems-ams
theorems-ams-extended
fix-cm
fixltx2e
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2cm
\topmargin 2cm
\rightmargin 2cm
\bottommargin 2cm
\headheight 2cm
\headsep 2cm
\footskip 2cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Project Notebook: Q-SVM
\end_layout

\begin_layout Section
Classical SVMs
\end_layout

\begin_layout Standard
The support vector machine framework (SVM) [Norvig] is currently the most
 popular approach for supervised learning.
 There are three properties that make SVMs attractive:
\end_layout

\begin_layout Enumerate
SVMs construct a maximum margin separator - a decision boundary with the
 largest possible distance to example points.
 This helps them generalize well.
\end_layout

\begin_layout Enumerate
SVMs create a linear separating hyperplate, but they have the ability to
 embed the data into a higher-dimensional space, using the so-called 
\series bold
kernel trick.
 
\series default
Oftern data che are not linearly separable in the original input space are
 easily separable in the higher-dimensional space.
 The high-dimensional linear separator is actually nonlinear in the original
 space.
 This means that the hypothesis space is greatly expanded over methods that
 use strictly linear representations.
\end_layout

\begin_layout Enumerate
SVMs are a non-parametric method: the models need to retain training examples
 and potentially need to store them all.
 In practice they often end up retaining only a small fraction of the number
 of examples, i.e.
 approximately as few as a small constant times the number of dimensions.
 Thus SVMs combine the advantages of nonparametric and parametric models:
 they have to flexibility to represent complex functions, but they are resistant
 to overfitting.
\end_layout

\begin_layout Standard
(Raschka) SVMs could be considered as an extension of the perceptron: using
 the perceptron algorithm, we minimize misclassification errors.
 In SVMs, our optimization objective is to maximize the margin.
\end_layout

\begin_layout Standard
The margin is defined as the distance between the separating hyperplane
 (decision boundary) and the training samples which are closest to this
 hyperplane, which are the so-called support vectors.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename ../img/cSVM1.png
	width 15cm

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
SVMs: selection of the decision boundary which maximzes the margin.
 ref.
 [Raschka]
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Maximum margin intuition:
\end_layout

\begin_layout Standard
To get an intuition about margin maximization, let's take a closer look
 at those 
\emph on
positive
\emph default
 and 
\emph on
negative
\emph default
 hyperplanes that are parallel to the decision boundary, which can be expressed
 as follows:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
w_{0}+\boldsymbol{w}^{T}\boldsymbol{x}_{+} & =1\\
w_{0}+\boldsymbol{w}^{T}\boldsymbol{x}_{-} & =-1
\end{align*}

\end_inset

If we subtract those two linear equations from each other, we get:
\begin_inset Formula 
\[
\boldsymbol{w}^{T}\left(\boldsymbol{x}_{+}-\boldsymbol{x}_{-}\right)=2
\]

\end_inset

Normalizing the expression with the length of 
\begin_inset Formula $\boldsymbol{w}$
\end_inset

 we obtain (please note that 
\begin_inset Formula $w_{0}$
\end_inset

 shouldn't be included in the vector)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\boldsymbol{w}^{T}\left(\boldsymbol{x}_{+}-\boldsymbol{x}_{-}\right)}{\left\Vert \boldsymbol{w}\right\Vert }=\frac{2}{\left\Vert \boldsymbol{w}\right\Vert }
\]

\end_inset

The left expression can be interpreted as the distance between the positive
 and negative hyperplane.
 Now the objective function of the SVM becomes the maximization of this
 margin.
 Alternatively, we could minimize the reciprocal of the right expression
 using quadratic programming:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{\left\Vert \boldsymbol{w}\right\Vert ^{2}}{2}
\]

\end_inset


\end_layout

\begin_layout Subsection
Dealing with misclassification using slack variables
\end_layout

\begin_layout Standard
This method was introduced by Vladimir Vapnik in 1995 and let to the so-called
 soft-margin classification.
 The motivation for introducing the slack variable 
\begin_inset Formula $\xi$
\end_inset

 was that the linear constraints need to be relaxed for nonlinearly separable
 data to allow convergence of the optimization in the presence of misclassificat
ions under the appropriate cost penalization.
\end_layout

\begin_layout Standard
The positive-values slack variable is simply added to the linear constraints:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\begin{cases}
\boldsymbol{w}^{T}\boldsymbol{x}^{\left(i\right)}\geq1 & \text{if }\,y^{\left(i\right)}=1-\xi^{\left(i\right)}\\
\boldsymbol{w}^{T}\boldsymbol{x}^{\left(i\right)}\leq-1 & \text{if }\,y^{\left(i\right)}=1+\xi^{\left(i\right)}
\end{cases}
\]

\end_inset

so the new objective to be minimized (subject to the preciding constraints)
 becomes:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\frac{1}{2}\left\Vert \boldsymbol{w}\right\Vert ^{2}+C\sum_{i}\xi^{\left(i\right)}
\]

\end_inset

where 
\begin_inset Formula $C$
\end_inset

 is an hyper-parameter in order to control the penalty for misclassification
\end_layout

\begin_layout Section
Kernel-based SVMs
\end_layout

\begin_layout Standard
[Raschka] Another reason why SVMs enjoy high popularity among machine learning
 practitioners is that they can be easily kernelized to solve nonlinear
 classification problems.
 Considering the infamous XOR classification problem (linear function has
 VC dimension of 3 in a 2-dim feature space: that model could shatter three
 points, but not four.)
\end_layout

\begin_layout Standard
We would not be able to separate samples frome the positive and negative
 class very well using a linear hyperplane as the decision boundary via
 the linear SVM model that we discussed earlier.
\end_layout

\begin_layout Standard
The basic idea behind kernel methods to deal with such linearly inseparable
 data is to create linear combinations of the original feature to project
 them onto a higher dimensional space via a mapping function 
\begin_inset Formula $\phi\left(\cdot\right)$
\end_inset

 where it becomes linearly separable.
\end_layout

\begin_layout Standard
However, one problem with this mapping approach is that the construction
 of the new features is computationally very expensive, especially if we
 are dealing with high-dimensional.data.
 This is where the so-called kernel-trick comes into play.
 Although we didn't go into much detail about how to solve the quadratic
 programming task to train an SVM, in practice all we need is to replace
 the dot product with mapped feature dot product.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left(x^{\left(i\right)}\right)^{T}x^{\left(i\right)}\mapsto\phi\left(x^{\left(i\right)}^{T}\right)\phi\left(x^{\left(i\right)}\right)
\]

\end_inset

In order to save the expensive step of calculating this dot product between
 two points explicitly, we define a so-called kernel function:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
K\left(x^{\left(i\right)},x^{\left(j\right)}\right)=\phi\left(x^{\left(i\right)}{}^{T}\right)\phi\left(x^{\left(i\right)}\right)
\]

\end_inset

One of the most widely used kernels is the Radial Basis Function kernel
 (RBF kernel) or Gaussian kernel:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
K\left(x^{\left(i\right)},x^{\left(j\right)}\right)=\exp\left\{ -\frac{\left\Vert \boldsymbol{x}^{\left(i\right)}-\boldsymbol{x}^{\left(j\right)}\right\Vert ^{2}}{2\sigma^{2}}\right\} \equiv\exp\left\{ -\gamma\left\Vert \boldsymbol{x}^{\left(i\right)}-\boldsymbol{x}^{\left(j\right)}\right\Vert ^{2}\right\} 
\]

\end_inset

where 
\begin_inset Formula $\gamma=\frac{1}{2\sigma^{2}}$
\end_inset

 is a hyper-parameter to be optimized.
\end_layout

\begin_layout Section
Quantum SVM
\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "RML-2014"
key "RML-2014"
literal "false"

\end_inset


\series bold
\shape smallcaps
Rebentrost, P., Mohseni, M., & Lloyd, S.
\series default
\shape default

\begin_inset Newline newline
\end_inset


\emph on
Quantum Support Vector Machine for Big Data Classification.
\emph default

\begin_inset Newline newline
\end_inset


\series bold
Physical Review Letters
\series default
, 113(13) - (
\series bold
2014
\series default
)
\begin_inset Newline newline
\end_inset


\begin_inset Flex URL
status open

\begin_layout Plain Layout

doi:10.1103/physrevlett.113.130503
\end_layout

\end_inset


\end_layout

\begin_layout Bibliography
\begin_inset CommandInset bibitem
LatexCommand bibitem
label "Norvig"
key "Norvig"
literal "false"

\end_inset

Russell, Stuart J., and Peter Norvig.
 Artificial Intelligence A Modern Approach.
 2016.
\end_layout

\end_body
\end_document
